{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "UAfohVN1e1tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4866, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4866, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])"
      ],
      "metadata": {
        "id": "DkCkBUlPfHUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_data,  batch_size=64, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "C4v0Pv_9fOIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4513a157-81ca-4555-aaa8-f4c3737ab261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:14<00:00, 11.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = train_data.classes\n",
        "print(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIOGEKFUABMI",
        "outputId": "c8ceb000-8a12-474a-ad1b-e95aea630d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6zASbIHABPU",
        "outputId": "92489b3f-829d-4f38-b70b-fd57c8f5fd70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Лучшие параметры: {'lr': 0.00021318096016397308, 'batch_size': 32}"
      ],
      "metadata": {
        "id": "-qaCfbtn65Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16_CIFAR(nn.Module):\n",
        "    def __init__(self, num_classes=100, dropout=0.325):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1),   nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),  nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wzq4qxy7ABYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = VGG16_CIFAR().to(device)"
      ],
      "metadata": {
        "id": "SUNbUgGUABbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.000117, weight_decay=0.000110)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ],
      "metadata": {
        "id": "x5luamMNACSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=300\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f'Эпоха {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDj_dwT_ACVw",
        "outputId": "b7e311e2-b7a4-4d93-a0d2-a55ff99a5367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Эпоха 1/300, Loss: 3.9234\n",
            "Эпоха 2/300, Loss: 3.3302\n",
            "Эпоха 3/300, Loss: 2.8764\n",
            "Эпоха 4/300, Loss: 2.5645\n",
            "Эпоха 5/300, Loss: 2.3329\n",
            "Эпоха 6/300, Loss: 2.1525\n",
            "Эпоха 7/300, Loss: 2.0014\n",
            "Эпоха 8/300, Loss: 1.8775\n",
            "Эпоха 9/300, Loss: 1.7777\n",
            "Эпоха 10/300, Loss: 1.6701\n",
            "Эпоха 11/300, Loss: 1.5833\n",
            "Эпоха 12/300, Loss: 1.5128\n",
            "Эпоха 13/300, Loss: 1.4417\n",
            "Эпоха 14/300, Loss: 1.3786\n",
            "Эпоха 15/300, Loss: 1.3102\n",
            "Эпоха 16/300, Loss: 1.2521\n",
            "Эпоха 17/300, Loss: 1.2022\n",
            "Эпоха 18/300, Loss: 1.1502\n",
            "Эпоха 19/300, Loss: 1.0985\n",
            "Эпоха 20/300, Loss: 1.0578\n",
            "Эпоха 21/300, Loss: 1.0128\n",
            "Эпоха 22/300, Loss: 0.9683\n",
            "Эпоха 23/300, Loss: 0.9247\n",
            "Эпоха 24/300, Loss: 0.8848\n",
            "Эпоха 25/300, Loss: 0.8469\n",
            "Эпоха 26/300, Loss: 0.8089\n",
            "Эпоха 27/300, Loss: 0.7752\n",
            "Эпоха 28/300, Loss: 0.7409\n",
            "Эпоха 29/300, Loss: 0.7063\n",
            "Эпоха 30/300, Loss: 0.6728\n",
            "Эпоха 31/300, Loss: 0.6364\n",
            "Эпоха 32/300, Loss: 0.6179\n",
            "Эпоха 33/300, Loss: 0.5801\n",
            "Эпоха 34/300, Loss: 0.5548\n",
            "Эпоха 35/300, Loss: 0.5216\n",
            "Эпоха 36/300, Loss: 0.5019\n",
            "Эпоха 37/300, Loss: 0.4737\n",
            "Эпоха 38/300, Loss: 0.4475\n",
            "Эпоха 39/300, Loss: 0.4267\n",
            "Эпоха 40/300, Loss: 0.4088\n",
            "Эпоха 41/300, Loss: 0.3870\n",
            "Эпоха 42/300, Loss: 0.3617\n",
            "Эпоха 43/300, Loss: 0.3350\n",
            "Эпоха 44/300, Loss: 0.3268\n",
            "Эпоха 45/300, Loss: 0.3108\n",
            "Эпоха 46/300, Loss: 0.2868\n",
            "Эпоха 47/300, Loss: 0.2700\n",
            "Эпоха 48/300, Loss: 0.2573\n",
            "Эпоха 49/300, Loss: 0.2409\n",
            "Эпоха 50/300, Loss: 0.2296\n",
            "Эпоха 51/300, Loss: 0.2107\n",
            "Эпоха 52/300, Loss: 0.2027\n",
            "Эпоха 53/300, Loss: 0.1892\n",
            "Эпоха 54/300, Loss: 0.1845\n",
            "Эпоха 55/300, Loss: 0.1712\n",
            "Эпоха 56/300, Loss: 0.1576\n",
            "Эпоха 57/300, Loss: 0.1488\n",
            "Эпоха 58/300, Loss: 0.1496\n",
            "Эпоха 59/300, Loss: 0.1317\n",
            "Эпоха 60/300, Loss: 0.1221\n",
            "Эпоха 61/300, Loss: 0.1151\n",
            "Эпоха 62/300, Loss: 0.1095\n",
            "Эпоха 63/300, Loss: 0.0997\n",
            "Эпоха 64/300, Loss: 0.0992\n",
            "Эпоха 65/300, Loss: 0.0898\n",
            "Эпоха 66/300, Loss: 0.0814\n",
            "Эпоха 67/300, Loss: 0.0784\n",
            "Эпоха 68/300, Loss: 0.0732\n",
            "Эпоха 69/300, Loss: 0.0654\n",
            "Эпоха 70/300, Loss: 0.0636\n",
            "Эпоха 71/300, Loss: 0.0573\n",
            "Эпоха 72/300, Loss: 0.0556\n",
            "Эпоха 73/300, Loss: 0.0511\n",
            "Эпоха 74/300, Loss: 0.0485\n",
            "Эпоха 75/300, Loss: 0.0431\n",
            "Эпоха 76/300, Loss: 0.0408\n",
            "Эпоха 77/300, Loss: 0.0399\n",
            "Эпоха 78/300, Loss: 0.0318\n",
            "Эпоха 79/300, Loss: 0.0326\n",
            "Эпоха 80/300, Loss: 0.0292\n",
            "Эпоха 81/300, Loss: 0.0271\n",
            "Эпоха 82/300, Loss: 0.0284\n",
            "Эпоха 83/300, Loss: 0.0250\n",
            "Эпоха 84/300, Loss: 0.0221\n",
            "Эпоха 85/300, Loss: 0.0221\n",
            "Эпоха 86/300, Loss: 0.0210\n",
            "Эпоха 87/300, Loss: 0.0183\n",
            "Эпоха 88/300, Loss: 0.0164\n",
            "Эпоха 89/300, Loss: 0.0165\n",
            "Эпоха 90/300, Loss: 0.0161\n",
            "Эпоха 91/300, Loss: 0.0145\n",
            "Эпоха 92/300, Loss: 0.0145\n",
            "Эпоха 93/300, Loss: 0.0151\n",
            "Эпоха 94/300, Loss: 0.0123\n",
            "Эпоха 95/300, Loss: 0.0129\n",
            "Эпоха 96/300, Loss: 0.0128\n",
            "Эпоха 97/300, Loss: 0.0125\n",
            "Эпоха 98/300, Loss: 0.0131\n",
            "Эпоха 99/300, Loss: 0.0137\n",
            "Эпоха 100/300, Loss: 0.0129\n",
            "Эпоха 101/300, Loss: 0.0119\n",
            "Эпоха 102/300, Loss: 0.0131\n",
            "Эпоха 103/300, Loss: 0.0125\n",
            "Эпоха 104/300, Loss: 0.0128\n",
            "Эпоха 105/300, Loss: 0.0121\n",
            "Эпоха 106/300, Loss: 0.0119\n",
            "Эпоха 107/300, Loss: 0.0116\n",
            "Эпоха 108/300, Loss: 0.0130\n",
            "Эпоха 109/300, Loss: 0.0125\n",
            "Эпоха 110/300, Loss: 0.0124\n",
            "Эпоха 111/300, Loss: 0.0133\n",
            "Эпоха 112/300, Loss: 0.0147\n",
            "Эпоха 113/300, Loss: 0.0127\n",
            "Эпоха 114/300, Loss: 0.0139\n",
            "Эпоха 115/300, Loss: 0.0140\n",
            "Эпоха 116/300, Loss: 0.0158\n",
            "Эпоха 117/300, Loss: 0.0162\n",
            "Эпоха 118/300, Loss: 0.0163\n",
            "Эпоха 119/300, Loss: 0.0173\n",
            "Эпоха 120/300, Loss: 0.0178\n",
            "Эпоха 121/300, Loss: 0.0208\n",
            "Эпоха 122/300, Loss: 0.0217\n",
            "Эпоха 123/300, Loss: 0.0229\n",
            "Эпоха 124/300, Loss: 0.0254\n",
            "Эпоха 125/300, Loss: 0.0276\n",
            "Эпоха 126/300, Loss: 0.0297\n",
            "Эпоха 127/300, Loss: 0.0296\n",
            "Эпоха 128/300, Loss: 0.0344\n",
            "Эпоха 129/300, Loss: 0.0387\n",
            "Эпоха 130/300, Loss: 0.0408\n",
            "Эпоха 131/300, Loss: 0.0399\n",
            "Эпоха 132/300, Loss: 0.0443\n",
            "Эпоха 133/300, Loss: 0.0467\n",
            "Эпоха 134/300, Loss: 0.0537\n",
            "Эпоха 135/300, Loss: 0.0538\n",
            "Эпоха 136/300, Loss: 0.0564\n",
            "Эпоха 137/300, Loss: 0.0631\n",
            "Эпоха 138/300, Loss: 0.0677\n",
            "Эпоха 139/300, Loss: 0.0717\n",
            "Эпоха 140/300, Loss: 0.0790\n",
            "Эпоха 141/300, Loss: 0.0735\n",
            "Эпоха 142/300, Loss: 0.0817\n",
            "Эпоха 143/300, Loss: 0.0878\n",
            "Эпоха 144/300, Loss: 0.0896\n",
            "Эпоха 145/300, Loss: 0.0949\n",
            "Эпоха 146/300, Loss: 0.1028\n",
            "Эпоха 147/300, Loss: 0.1051\n",
            "Эпоха 148/300, Loss: 0.1107\n",
            "Эпоха 149/300, Loss: 0.1153\n",
            "Эпоха 150/300, Loss: 0.1185\n",
            "Эпоха 151/300, Loss: 0.1220\n",
            "Эпоха 152/300, Loss: 0.1196\n",
            "Эпоха 153/300, Loss: 0.1399\n",
            "Эпоха 154/300, Loss: 0.1353\n",
            "Эпоха 155/300, Loss: 0.1413\n",
            "Эпоха 156/300, Loss: 0.1415\n",
            "Эпоха 157/300, Loss: 0.1461\n",
            "Эпоха 158/300, Loss: 0.1516\n",
            "Эпоха 159/300, Loss: 0.1586\n",
            "Эпоха 160/300, Loss: 0.1552\n",
            "Эпоха 161/300, Loss: 0.1607\n",
            "Эпоха 162/300, Loss: 0.1654\n",
            "Эпоха 163/300, Loss: 0.1697\n",
            "Эпоха 164/300, Loss: 0.1631\n",
            "Эпоха 165/300, Loss: 0.1818\n",
            "Эпоха 166/300, Loss: 0.1789\n",
            "Эпоха 167/300, Loss: 0.1798\n",
            "Эпоха 168/300, Loss: 0.1844\n",
            "Эпоха 169/300, Loss: 0.1793\n",
            "Эпоха 170/300, Loss: 0.1728\n",
            "Эпоха 171/300, Loss: 0.1915\n",
            "Эпоха 172/300, Loss: 0.1834\n",
            "Эпоха 173/300, Loss: 0.1901\n",
            "Эпоха 174/300, Loss: 0.1944\n",
            "Эпоха 175/300, Loss: 0.1872\n",
            "Эпоха 176/300, Loss: 0.1903\n",
            "Эпоха 177/300, Loss: 0.1976\n",
            "Эпоха 178/300, Loss: 0.1954\n",
            "Эпоха 179/300, Loss: 0.1939\n",
            "Эпоха 180/300, Loss: 0.1949\n",
            "Эпоха 181/300, Loss: 0.1976\n",
            "Эпоха 182/300, Loss: 0.1962\n",
            "Эпоха 183/300, Loss: 0.1957\n",
            "Эпоха 184/300, Loss: 0.1961\n",
            "Эпоха 185/300, Loss: 0.1845\n",
            "Эпоха 186/300, Loss: 0.1930\n",
            "Эпоха 187/300, Loss: 0.1945\n",
            "Эпоха 188/300, Loss: 0.1905\n",
            "Эпоха 189/300, Loss: 0.1911\n",
            "Эпоха 190/300, Loss: 0.1855\n",
            "Эпоха 191/300, Loss: 0.1905\n",
            "Эпоха 192/300, Loss: 0.1853\n",
            "Эпоха 193/300, Loss: 0.1834\n",
            "Эпоха 194/300, Loss: 0.1842\n",
            "Эпоха 195/300, Loss: 0.1819\n",
            "Эпоха 196/300, Loss: 0.1830\n",
            "Эпоха 197/300, Loss: 0.1815\n",
            "Эпоха 198/300, Loss: 0.1761\n",
            "Эпоха 199/300, Loss: 0.1693\n",
            "Эпоха 200/300, Loss: 0.1771\n",
            "Эпоха 201/300, Loss: 0.1615\n",
            "Эпоха 202/300, Loss: 0.1670\n",
            "Эпоха 203/300, Loss: 0.1635\n",
            "Эпоха 204/300, Loss: 0.1633\n",
            "Эпоха 205/300, Loss: 0.1614\n",
            "Эпоха 206/300, Loss: 0.1519\n",
            "Эпоха 207/300, Loss: 0.1572\n",
            "Эпоха 208/300, Loss: 0.1556\n",
            "Эпоха 209/300, Loss: 0.1522\n",
            "Эпоха 210/300, Loss: 0.1492\n",
            "Эпоха 211/300, Loss: 0.1399\n",
            "Эпоха 212/300, Loss: 0.1518\n",
            "Эпоха 213/300, Loss: 0.1327\n",
            "Эпоха 214/300, Loss: 0.1424\n",
            "Эпоха 215/300, Loss: 0.1328\n",
            "Эпоха 216/300, Loss: 0.1337\n",
            "Эпоха 217/300, Loss: 0.1371\n",
            "Эпоха 218/300, Loss: 0.1286\n",
            "Эпоха 219/300, Loss: 0.1247\n",
            "Эпоха 220/300, Loss: 0.1142\n",
            "Эпоха 221/300, Loss: 0.1233\n",
            "Эпоха 222/300, Loss: 0.1173\n",
            "Эпоха 223/300, Loss: 0.1218\n",
            "Эпоха 224/300, Loss: 0.1095\n",
            "Эпоха 225/300, Loss: 0.1106\n",
            "Эпоха 226/300, Loss: 0.1058\n",
            "Эпоха 227/300, Loss: 0.0993\n",
            "Эпоха 228/300, Loss: 0.0980\n",
            "Эпоха 229/300, Loss: 0.1018\n",
            "Эпоха 230/300, Loss: 0.0984\n",
            "Эпоха 231/300, Loss: 0.0863\n",
            "Эпоха 232/300, Loss: 0.0890\n",
            "Эпоха 233/300, Loss: 0.0931\n",
            "Эпоха 234/300, Loss: 0.0810\n",
            "Эпоха 235/300, Loss: 0.0839\n",
            "Эпоха 236/300, Loss: 0.0761\n",
            "Эпоха 237/300, Loss: 0.0791\n",
            "Эпоха 238/300, Loss: 0.0759\n",
            "Эпоха 239/300, Loss: 0.0722\n",
            "Эпоха 240/300, Loss: 0.0703\n",
            "Эпоха 241/300, Loss: 0.0685\n",
            "Эпоха 242/300, Loss: 0.0614\n",
            "Эпоха 243/300, Loss: 0.0622\n",
            "Эпоха 244/300, Loss: 0.0594\n",
            "Эпоха 245/300, Loss: 0.0543\n",
            "Эпоха 246/300, Loss: 0.0550\n",
            "Эпоха 247/300, Loss: 0.0511\n",
            "Эпоха 248/300, Loss: 0.0490\n",
            "Эпоха 249/300, Loss: 0.0493\n",
            "Эпоха 250/300, Loss: 0.0457\n",
            "Эпоха 251/300, Loss: 0.0417\n",
            "Эпоха 252/300, Loss: 0.0396\n",
            "Эпоха 253/300, Loss: 0.0408\n",
            "Эпоха 254/300, Loss: 0.0362\n",
            "Эпоха 255/300, Loss: 0.0332\n",
            "Эпоха 256/300, Loss: 0.0362\n",
            "Эпоха 257/300, Loss: 0.0321\n",
            "Эпоха 258/300, Loss: 0.0306\n",
            "Эпоха 259/300, Loss: 0.0246\n",
            "Эпоха 260/300, Loss: 0.0272\n",
            "Эпоха 261/300, Loss: 0.0260\n",
            "Эпоха 262/300, Loss: 0.0249\n",
            "Эпоха 263/300, Loss: 0.0240\n",
            "Эпоха 264/300, Loss: 0.0190\n",
            "Эпоха 265/300, Loss: 0.0177\n",
            "Эпоха 266/300, Loss: 0.0175\n",
            "Эпоха 267/300, Loss: 0.0183\n",
            "Эпоха 268/300, Loss: 0.0168\n",
            "Эпоха 269/300, Loss: 0.0156\n",
            "Эпоха 270/300, Loss: 0.0128\n",
            "Эпоха 271/300, Loss: 0.0124\n",
            "Эпоха 272/300, Loss: 0.0111\n",
            "Эпоха 273/300, Loss: 0.0122\n",
            "Эпоха 274/300, Loss: 0.0089\n",
            "Эпоха 275/300, Loss: 0.0104\n",
            "Эпоха 276/300, Loss: 0.0080\n",
            "Эпоха 277/300, Loss: 0.0082\n",
            "Эпоха 278/300, Loss: 0.0067\n",
            "Эпоха 279/300, Loss: 0.0058\n",
            "Эпоха 280/300, Loss: 0.0054\n",
            "Эпоха 281/300, Loss: 0.0053\n",
            "Эпоха 282/300, Loss: 0.0055\n",
            "Эпоха 283/300, Loss: 0.0054\n",
            "Эпоха 284/300, Loss: 0.0045\n",
            "Эпоха 285/300, Loss: 0.0038\n",
            "Эпоха 286/300, Loss: 0.0036\n",
            "Эпоха 287/300, Loss: 0.0037\n",
            "Эпоха 288/300, Loss: 0.0035\n",
            "Эпоха 289/300, Loss: 0.0030\n",
            "Эпоха 290/300, Loss: 0.0029\n",
            "Эпоха 291/300, Loss: 0.0030\n",
            "Эпоха 292/300, Loss: 0.0029\n",
            "Эпоха 293/300, Loss: 0.0029\n",
            "Эпоха 294/300, Loss: 0.0022\n",
            "Эпоха 295/300, Loss: 0.0027\n",
            "Эпоха 296/300, Loss: 0.0022\n",
            "Эпоха 297/300, Loss: 0.0027\n",
            "Эпоха 298/300, Loss: 0.0025\n",
            "Эпоха 299/300, Loss: 0.0023\n",
            "Эпоха 300/300, Loss: 0.0024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy на тесте: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "wXTXh9J5ACYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9999e745-c7d8-456b-d6af-64e456f10224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy на тесте: 70.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'cifar100_vgg.pth')"
      ],
      "metadata": {
        "id": "WkkD6jCvACb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna -q\n",
        "\n",
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Один раз загружаем данные с нормализацией\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])\n",
        "\n",
        "full_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# 45k train + 5k val (быстрее чем на всём 50k)\n",
        "train_dataset, val_dataset = random_split(full_train, [45000, 5000])\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def objective(trial):\n",
        "    # Параметры для поиска\n",
        "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
        "    dropout = trial.suggest_float('dropout', 0.2, 0.6)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Модель (добавил dropout из Optuna)\n",
        "    model = VGG16_CIFAR(dropout=dropout).to(device)  # используй мою версию VGG из прошлого ответа\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Быстрое обучение — 10-15 эпох хватает для оценки\n",
        "    for epoch in range(12):\n",
        "        model.train()\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Валидация\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            correct += (model(x).argmax(1) == y).sum().item()\n",
        "\n",
        "    accuracy = correct / len(val_dataset)\n",
        "    return accuracy\n",
        "\n",
        "# Запуск\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
        "study.optimize(objective, n_trials=30, timeout=None)  # ~3-6 часов на GPU\n",
        "\n",
        "print(\"Лучшие параметры:\", study.best_params)\n",
        "print(f\"Лучшая валидационная точность: {study.best_value*100:.2f}%\")"
      ],
      "metadata": {
        "id": "hAGFQnXbf8Yp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}